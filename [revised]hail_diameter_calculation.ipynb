{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88ba58dc-af10-405b-97a2-ad5c0b905179",
   "metadata": {},
   "source": [
    "# Preliminary Notes\n",
    "\n",
    "This script was developed to calculate hydrometeor particles for the doctoral dissertation of **Sari (2025)**, titled *A Study of Hailstorms in Maritime Tropical Area: Surabaya Hail Events, Indonesia*, at the University of Illinois Urbana–Champaign. The code has been applied in that study, and portions of it are already published in *JGR: Atmospheres* as *Hailstorm Events Over a Maritime Tropical Region: Storm Environments and Characteristics* (Sari & Lasher-Trapp, 2025).\n",
    "\n",
    "---\n",
    "\n",
    "All rights to this code are reserved by **Sari (2025)**.\n",
    "\n",
    "## WRF - Hail Diameter Calculation (Gamma-Distribution Approach)\n",
    "Originally, this code was adapted from the WRF Model, where it was implemented in Fortran under the file name `module_diag_nwp.f90`.  For more flexibility, the author reimplemented it in Python with relevant modifications, including the treatment of density, number thresholds, and other parameters.\n",
    "\n",
    "### Purpose\n",
    "Compute hailstone diameters throughout the model column and extract:\n",
    "- **HMAXK1**: maximum hail diameter at the **lowest** model level  \n",
    "- **HMAX2D**: maximum hail diameter **anywhere in the column**  \n",
    "- **Full vertical profile** of hail diameter at each grid point  \n",
    "\n",
    "---\n",
    "\n",
    "### Inputs\n",
    "Arrays with shape `(time, zh, yh, xh)`:\n",
    "- `qhl`, `qg` — hail and graupel **mixing ratios** $[\\mathrm{kg}/\\mathrm{kg}]$  \n",
    "- `chl`, `chw` — hail and graupel **number concentrations** $[\\#/\\mathrm{kg}]$  \n",
    "- $\\rho$ — **dry-air density** $[\\mathrm{kg}\\,\\mathrm{m}^{-3}]$  \n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "Total hail-phase **mass** and **number** are computed by combining hail and graupel and converting to volume densities:\n",
    "\n",
    "$$\n",
    "q_h^{\\ast} = (q_{hl}+q_g)\\,\\rho, \\qquad n_h^{\\ast} = (c_{hl}+c_{hw})\\,\\rho\n",
    "$$\n",
    "\n",
    "A **gamma size distribution** is assumed for hail:\n",
    "\n",
    "$$\n",
    "N(D) = N_0\\,D^{\\mu}\\,e^{-\\lambda D}\n",
    "$$\n",
    "\n",
    "with parameters\n",
    "\n",
    "$$\n",
    "\\lambda = \\left(\\frac{a\\,n_h^{\\ast}}{q_h^{\\ast}}\\right)^{1/b}, \\qquad \n",
    "a=\\frac{\\pi}{6}\\rho_h, \\quad b=3, \\quad \\mu=0\n",
    "$$\n",
    "\n",
    "where $\\rho_h$ is hail density (default $=900~\\mathrm{kg}\\,\\mathrm{m}^{-3}$).\n",
    "\n",
    "---\n",
    "\n",
    "### Discretization & Search\n",
    "- **51 logarithmic bins** spanning **0.5 mm** to **7.5 cm**.  \n",
    "- Geometric midpoints $D_g$ and widths $\\Delta D$ define discrete counts.  \n",
    "- A cumulative number integral is computed from large to small diameters.  \n",
    "- The hail size is taken where the cumulative number first exceeds a threshold:  \n",
    "\n",
    "$$\n",
    "\\sum N(D_g)\\,\\Delta D \\;\\ge\\; \\texttt{thresh\\_conc}\n",
    "$$\n",
    "\n",
    "- Default threshold: $thresh\\_conc = 3 \\times 10^{-4}$\n",
    "\n",
    "If the crossing occurs between bins, **linear interpolation** refines the diameter.\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "- `hail_profile` $(time, zh, yh, xh)$: hail diameter at each vertical level and grid cell  \n",
    "- `hail_max2d` $(time, yh, xh)$: column-maximum hail diameter (HMAX2D)  \n",
    "- `hail_maxk1` $(time, yh, xh)$: maximum hail diameter at the lowest level (HMAXK1)  \n",
    "\n",
    "---\n",
    "\n",
    "### Constants & Defaults\n",
    "- Hail bulk density: $\\rho_h = 900\\ \\mathrm{kg}\\,\\mathrm{m}^{-3}$  \n",
    "- Gamma parameters: $a=\\dfrac{\\pi}{6}\\rho_h,\\; b=3,\\; \\mu=0$  \n",
    "- Floors for stability:  \n",
    "  - $q_h^{\\ast} \\ge 1\\times10^{-12}$  \n",
    "  - $n_h^{\\ast} \\ge 1\\times10^{-8}$  \n",
    "- Bin range: $0.5~\\mathrm{mm}$ – $7.5~\\mathrm{cm}$  \n",
    "\n",
    "---\n",
    "\n",
    "### Notes & Assumptions\n",
    "- Hail and graupel are **combined** into a single rimed-ice category.  \n",
    "- Threshold logic identifies the **largest representative size**.  \n",
    "- Output diameters are in **meters**.  \n",
    "- Triple nested loops are used per time step; for large domains, vectorization or acceleration may be beneficial.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8df946-9286-46bd-a062-b57df6cfa6a9",
   "metadata": {},
   "source": [
    "### Hail Size Calculation Design For CM1 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4faf90c-45d8-4af9-87d0-924d2494e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "\n",
    "import numpy as np \n",
    "import xarray as xr \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker\n",
    "import matplotlib.ticker as tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29779ff-d5cb-4721-88e4-9fde841fba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after code environment reset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_hail_diameter_profile(qhl, chl, qg, chw, rho):\n",
    "    \"\"\"\n",
    "    Calculates hail diameter profile over vertical layers from atmospheric data.\n",
    "\n",
    "    Parameters:\n",
    "    - qhl, qg: Mixing ratios of hail and graupel (kg/kg), shape (time, zh, yh, xh)\n",
    "    - chl, chw: Number concentrations of hail and graupel (#/kg), shape (time, zh, yh, xh)\n",
    "    - rho: Dry-air density (kg/m^3), shape (time, zh, yh, xh)\n",
    "\n",
    "    Returns:\n",
    "    - hail_profile: Hail diameter at each vertical level, shape (time, zh, yh, xh)\n",
    "    - hail_max2d: Maximum hail size across vertical levels, shape (time, yh, xh)\n",
    "    - hail_maxk1: Hail size at lowest vertical level, shape (time, yh, xh)\n",
    "    \"\"\"\n",
    "    time, nk, ny, nx = qhl.shape\n",
    "    hail_profile = np.zeros((time, nk, ny, nx))\n",
    "    hail_max2d = np.zeros((time, ny, nx))\n",
    "    hail_maxk1 = np.zeros((time, ny, nx))\n",
    "\n",
    "    # Constants\n",
    "    xrho_h = 900.0\n",
    "    xam_h = np.pi / 6.0 * xrho_h\n",
    "    xbm_h = 3.0\n",
    "    xmu_h = 0.0\n",
    "    thresh_conc = 0.0003 #0.001\n",
    "\n",
    "    # Bins for hail size\n",
    "    xxDx = np.zeros(51)\n",
    "    xxDx[0] = 500e-6\n",
    "    xxDx[-1] = 0.075\n",
    "    for n in range(1, 50):\n",
    "        xxDx[n] = np.exp((n) / 50.0 * np.log(xxDx[-1] / xxDx[0]) + np.log(xxDx[0]))\n",
    "    xxDg = np.sqrt(xxDx[:-1] * xxDx[1:])\n",
    "    xdtg = xxDx[1:] - xxDx[:-1]\n",
    "\n",
    "    # Loop over time with progress bar\n",
    "    for t in tqdm(range(time), desc=\"⏳ Calculating hail diameter\"):\n",
    "        temp_qh = np.maximum(1e-12, (qhl[t] + qg[t]) * rho[t])\n",
    "        temp_nh = np.maximum(1e-8, (chl[t] + chw[t]) * rho[t])\n",
    "\n",
    "        for k in range(nk):\n",
    "            for j in range(ny):\n",
    "                for i in range(nx):\n",
    "                    if temp_qh[k, j, i] < 1e-6:\n",
    "                        continue\n",
    "\n",
    "                    lamh = (xam_h * temp_nh[k, j, i] / temp_qh[k, j, i])**(1.0 / xbm_h)\n",
    "                    N0_h = temp_nh[k, j, i]\n",
    "\n",
    "                    sum_nh = 0.0\n",
    "                    sum_t = 0.0\n",
    "                    ng = 49\n",
    "                    while ng >= 0:\n",
    "                        f_d = N0_h * xxDg[ng]**xmu_h * np.exp(-lamh * xxDg[ng]) * xdtg[ng]\n",
    "                        sum_nh += f_d\n",
    "                        if sum_nh > thresh_conc:\n",
    "                            break\n",
    "                        sum_t = sum_nh\n",
    "                        ng -= 1\n",
    "\n",
    "                    if ng >= 49:\n",
    "                        hail_max = xxDg[-1]\n",
    "                    elif ng >= 0 and xxDg[ng + 1] > 1e-3:\n",
    "                        hail_max = xxDg[ng] - (sum_nh - thresh_conc) / (sum_nh - sum_t) * (xxDg[ng] - xxDg[ng + 1])\n",
    "                    else:\n",
    "                        hail_max = 1e-4\n",
    "\n",
    "                    hail_profile[t, k, j, i] = hail_max  # Store full vertical profile\n",
    "                    if k == 0:\n",
    "                        hail_maxk1[t, j, i] = max(hail_maxk1[t, j, i], hail_max)\n",
    "                    hail_max2d[t, j, i] = max(hail_max2d[t, j, i], hail_max)\n",
    "\n",
    "    return hail_profile, hail_max2d, hail_maxk1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de174a5e-db67-4b86-b1b3-6b31792744b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open Data\n",
    "\n",
    "def open_and_extract_multiple(files, variable_map):\n",
    "    \"\"\"\n",
    "    Opens multiple NetCDF files, extracts, and renames variables.\n",
    "\n",
    "    Parameters:\n",
    "    - files (list of str): List of file paths to NetCDF files.\n",
    "    - variable_map (dict): A dictionary mapping desired variable names to dataset keys.\n",
    "\n",
    "    Returns:\n",
    "    - list of dict: A list of dictionaries, each containing renamed variables from a file.\n",
    "    \"\"\"\n",
    "    all_variables = []\n",
    "\n",
    "    for file_path in files:\n",
    "        dataset = xr.open_dataset(file_path)\n",
    "        variables = {new_name: dataset[old_name] for new_name, old_name in variable_map.items()}\n",
    "        all_variables.append(variables)\n",
    "\n",
    "    return all_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04fb57b-32da-4fa4-9a75-9a752a4adc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Directory and file paths\n",
    "dir = '# add your directory here'\n",
    "files = [\n",
    "    dir + '# add your netcdf file here', #1\n",
    "\n",
    "# Variable mapping\n",
    "variable_map = {\n",
    "    'xh': 'xh',\n",
    "    'yh': 'yh',\n",
    "    'z': 'zf',\n",
    "    'time': 'time',\n",
    "    'cp': 'cape',\n",
    "    'cn': 'cin',\n",
    "    'u': 'uinterp',\n",
    "    'v': 'vinterp',\n",
    "    'w': 'winterp',\n",
    "    'qv': 'qv',\n",
    "    'qc': 'qc',\n",
    "    'qr': 'qr',\n",
    "    'qg': 'qg',\n",
    "    'qh': 'qhl',\n",
    "    'ref': 'dbz', \n",
    "    'chw': 'chw',\n",
    "    'chl': 'chl',\n",
    "    'crw': 'crw', \n",
    "    'th': 'th', \n",
    "    'p' : 'prs', \n",
    "    'rho' : 'rho'\n",
    "}\n",
    "\n",
    "# Open all files into separate datasets\n",
    "datasets = [xr.open_dataset(file) for file in files]\n",
    "\n",
    "# Extract and rename variables for each file\n",
    "all_variables = []\n",
    "for idx, dataset in enumerate(datasets, start=1):\n",
    "    variables = {f\"{new_name}_{idx}\": dataset[old_name] for new_name, old_name in variable_map.items()}\n",
    "    all_variables.append(variables)\n",
    "\n",
    "# Unpack variables into individual references\n",
    "for idx, variables in enumerate(all_variables, start=1):\n",
    "    globals().update({f\"{name}\": var for name, var in variables.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdf72d-e01d-4a9c-9583-aa346547f323",
   "metadata": {},
   "source": [
    "### Looping all data calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e424204a-ac64-4b70-a608-2791b764d57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing set 12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⏳ Calculating hail diameter: 100%|██████████| 60/60 [7:10:34<00:00, 430.57s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hail data for set 12 ✅\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "# Output folder\n",
    "output_dir = \"# add directory for output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop over dataset versions (1 to 4, adjust if needed)\n",
    "# for idx in range(1, 5):\n",
    "for idx in [12]:\n",
    "    print(f\"Processing set {idx}...\")\n",
    "\n",
    "    # Get each variable dynamically\n",
    "    qh = globals()[f\"qh_{idx}\"].isel(time=slice(20, 80)).load()\n",
    "    qg = globals()[f\"qg_{idx}\"].isel(time=slice(20, 80)).load()\n",
    "    chl = globals()[f\"chl_{idx}\"].isel(time=slice(20, 80)).load()\n",
    "    chw = globals()[f\"chw_{idx}\"].isel(time=slice(20, 80)).load()\n",
    "    rho = globals()[f\"rho_{idx}\"].isel(time=slice(20, 80)).load()\n",
    "\n",
    "    # Run the hail diameter calculation\n",
    "    hmxprf, hmx2d, hmxk1 = calculate_hail_diameter_profile(\n",
    "        qh.values, chl.values, qg.values, chw.values, rho.values\n",
    "    )\n",
    "\n",
    "    # Wrap into DataArrays\n",
    "    hail_max2d_da = xr.DataArray(\n",
    "        hmx2d,\n",
    "        dims=[\"time\", \"yh\", \"xh\"],\n",
    "        coords={\"time\": qh.time, \"yh\": qh.yh, \"xh\": qh.xh},\n",
    "        name=\"hail_max2d\",\n",
    "        attrs={\n",
    "            \"long_name\": \"Maximum hail diameter at any vertical level\",\n",
    "            \"units\": \"m\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hail_maxk1_da = xr.DataArray(\n",
    "        hmxk1,\n",
    "        dims=[\"time\", \"yh\", \"xh\"],\n",
    "        coords={\"time\": qh.time, \"yh\": qh.yh, \"xh\": qh.xh},\n",
    "        name=\"hail_maxk1\",\n",
    "        attrs={\n",
    "            \"long_name\": \"Maximum hail diameter at surface level\",\n",
    "            \"units\": \"m\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    hail_profile_da = xr.DataArray(\n",
    "        hmxprf,\n",
    "        dims=[\"time\", \"zh\", \"yh\", \"xh\"],\n",
    "        coords={\"time\": qh.time, \"zh\": qh.zh, \"yh\": qh.yh, \"xh\": qh.xh},\n",
    "        name=\"hail_diameter_profile\",\n",
    "        attrs={\n",
    "            \"long_name\": \"Hail diameter at each vertical level\",\n",
    "            \"units\": \"m\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save as NetCDF\n",
    "    hail_max2d_da.to_netcdf(f\"{output_dir}/hmx2d_set{idx}_new_netcape.nc\")\n",
    "    #hail_maxk1_da.to_netcdf(f\"{output_dir}/hmxk1_d7_set{idx}_t20t80_tc00003_1.nc\")\n",
    "    hail_profile_da.to_netcdf(f\"{output_dir}/hmxprf_set{idx}_new_netcape.nc\")\n",
    "\n",
    "    print(f\"Saved hail data for set {idx} ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9e57e-ef9c-4de8-9c75-38c209865cf4",
   "metadata": {},
   "source": [
    "## Hydrometeor Particles Calculation with simple approach\n",
    "\n",
    "### Mass–Diameter Relationship for Hydrometeors\n",
    "\n",
    "This method is based on a simple **mass–diameter relationship** using:\n",
    "\n",
    "$$\n",
    "m = \\frac{q}{n}\n",
    "$$\n",
    "\n",
    "> where:\n",
    "> $m$ = average mass per particle \\[kg]\n",
    "> $q$ = mass mixing ratio \\[kg/kg]\n",
    "> $n$ = number concentration \\[#/kg]\n",
    "\n",
    "---\n",
    "\n",
    "Assuming **spherical particles**, the volume–mass relation is:\n",
    "\n",
    "$$\n",
    "m = \\frac{\\pi}{6} \\rho_p D^3 \\quad \\Rightarrow \\quad D = \\left( \\frac{6m}{\\pi \\rho_p} \\right)^{1/3}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\rho_p$ = particle material density \\[kg/m³]\n",
    "* $D$ = diameter \\[m]\n",
    "\n",
    "---\n",
    "\n",
    "### Suggested Hydrometeor Densities\n",
    "\n",
    "Use realistic values for $\\rho_p$ depending on the particle type:\n",
    "\n",
    "* **Rain**: \\~1000 kg/m³\n",
    "* **Graupel**: \\~400–500 kg/m³\n",
    "* **Hail**: \\~700–900 kg/m³\n",
    "\n",
    "---\n",
    "\n",
    "### Final Formula (Diameter in meters)\n",
    "\n",
    "$$\n",
    "D = \\left( \\frac{6 \\cdot \\frac{q}{n}}{\\pi \\cdot \\rho_p} \\right)^{1/3}\n",
    "$$\n",
    "\n",
    "Or simplified:\n",
    "\n",
    "$$\n",
    "D = \\left( \\frac{6q}{n \\pi \\rho_p} \\right)^{1/3}\n",
    "$$\n",
    "\n",
    "$\\rho_p$ can be also defined as appropriate for each hydrometeor type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc044e8f-d0fa-458c-96a1-35fceec2a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_d(q, n, rho_p):\n",
    "    \"\"\"\n",
    "    Compute hydrometeor equivalent spherical diameter from \n",
    "    mixing ratio and number concentration.\n",
    "\n",
    "    Parameters:\n",
    "    - q: Mass mixing ratio of hydrometeor [kg/kg]\n",
    "    - n: Number concentration of hydrometeor [#/kg]\n",
    "    - rho_p: Prescribed bulk density of the hydrometeor [kg/m³]\n",
    "\n",
    "    Returns:\n",
    "    - D: xarray.DataArray of equivalent spherical diameters [m],\n",
    "         same dimensions as input q and n\n",
    "\n",
    "    Notes:\n",
    "    - Average particle mass is computed as m = q / n.\n",
    "    - Diameters are estimated assuming spherical particles:\n",
    "          D = ((6 * m) / (π * ρ_p))^(1/3)\n",
    "    - NaNs are assigned where n ≤ 0.\n",
    "    \"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        m = xr.where(n > 0, q / n, np.nan)\n",
    "        D = ((6 * m) / (np.pi * rho_p))**(1/3)\n",
    "    return D\n",
    "\n",
    "\n",
    "def calculate_hydrometeor_diameters(ds):\n",
    "    \"\"\"\n",
    "    Calculates particle diameters for rain, graupel, and hail \n",
    "    using a simple mass–diameter relationship.\n",
    "\n",
    "    Parameters:\n",
    "    - ds: xarray.Dataset containing:\n",
    "        - qr, nr: Rain mixing ratio (kg/kg) and number concentration (#/kg)\n",
    "        - qg, ng: Graupel mixing ratio (kg/kg) and number concentration (#/kg)\n",
    "        - qh, nh: Hail mixing ratio (kg/kg) and number concentration (#/kg)\n",
    "\n",
    "    Returns:\n",
    "    - diameter_raindrop: DataArray of raindrop diameters [m], same dims as input\n",
    "    - diameter_graupel: DataArray of graupel diameters [m], same dims as input\n",
    "    - diameter_hail: DataArray of hail diameters [m], same dims as input\n",
    "\n",
    "    Notes:\n",
    "    - Prescribed bulk densities:\n",
    "        * Rain = 1000 kg/m³\n",
    "        * Graupel = 500 kg/m³ or can be also modified use direct grauepl density from CM1 output\n",
    "        * Hail = 900 kg/m³\n",
    "    - Computation is performed separately at each time step \n",
    "      and concatenated into full time series.\n",
    "    - Outputs include variable names and metadata (units, long_name).\n",
    "    \"\"\"\n",
    "    # Prescribed hydrometeor densities [kg/m³]\n",
    "    rho_rain = 1000.0\n",
    "    rho_graupel = 500.0\n",
    "    rho_hail = 900.0\n",
    "\n",
    "    qr = ds[\"qr\"]\n",
    "    nr = ds[\"nr\"]\n",
    "    qg = ds[\"qg\"]\n",
    "    ng = ds[\"ng\"]\n",
    "    qh = ds[\"qh\"]\n",
    "    nh = ds[\"nh\"]\n",
    "\n",
    "    time_dim = ds.sizes[\"time\"]\n",
    "    diam_rain = []\n",
    "    diam_graupel = []\n",
    "    diam_hail = []\n",
    "\n",
    "    for t in tqdm(range(time_dim), desc=\"Calculating diameters over time\"):\n",
    "        d_rain = compute_d(qr.isel(time=t), nr.isel(time=t), rho_rain)\n",
    "        d_grau = compute_d(qg.isel(time=t), ng.isel(time=t), rho_graupel)\n",
    "        d_hail = compute_d(qh.isel(time=t), nh.isel(time=t), rho_hail)\n",
    "\n",
    "        diam_rain.append(d_rain)\n",
    "        diam_graupel.append(d_grau)\n",
    "        diam_hail.append(d_hail)\n",
    "\n",
    "    # Stack back into DataArray with time\n",
    "    rain_d_da = xr.concat(diam_rain, dim=\"time\")\n",
    "    graupel_d_da = xr.concat(diam_graupel, dim=\"time\")\n",
    "    hail_d_da = xr.concat(diam_hail, dim=\"time\")\n",
    "\n",
    "    rain_d_da.name = \"diameter_raindrop\"\n",
    "    graupel_d_da.name = \"diameter_graupel\"\n",
    "    hail_d_da.name = \"diameter_hail\"\n",
    "\n",
    "    rain_d_da.attrs = {'units': 'm', 'long_name': 'Raindrop diameter'}\n",
    "    graupel_d_da.attrs = {'units': 'm', 'long_name': 'Graupel diameter'}\n",
    "    hail_d_da.attrs = {'units': 'm', 'long_name': 'Hail diameter'}\n",
    "\n",
    "    return rain_d_da, graupel_d_da, hail_d_da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63d963-75a2-4eea-95b5-db4fdf642e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Input/Output Setup ===\n",
    "\n",
    "input_dir = \"# add your input directory here\"\n",
    "output_dir = \"# add your output directory here\"\n",
    "\n",
    "selected_sets = [2, 12, 11, 8, 9, 10, 5]\n",
    "file_template = \"filtered_w10_new_set{}.nc\" #set according your nc files\n",
    "\n",
    "for i in selected_sets:\n",
    "    path_in = os.path.join(input_dir, file_template.format(i))\n",
    "    path_out = os.path.join(output_dir, f\"filtered_w10_new_dia_set{i}.nc\")\n",
    "\n",
    "    if os.path.exists(path_in):\n",
    "        print(f\"\\nProcessing set {i}...\")\n",
    "        ds = xr.open_dataset(path_in)\n",
    "\n",
    "        rain_d, graupel_d, hail_d = calculate_hydrometeor_diameters(ds)\n",
    "\n",
    "        # Merge and save\n",
    "        ds_out = ds.copy()\n",
    "        ds_out[\"diameter_raindrop\"] = rain_d\n",
    "        ds_out[\"diameter_graupel\"] = graupel_d\n",
    "        ds_out[\"diameter_hail\"] = hail_d\n",
    "\n",
    "        ds_out.to_netcdf(path_out)\n",
    "        print(f\"Saved: {path_out}\")\n",
    "    else:\n",
    "        print(f\"File not found: {path_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd7891-e410-4f99-b954-4d542b272450",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the statistics\n",
    "\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# === Input Directory and File Pattern ===\n",
    "input_dir = \"# add your input directory here\"\n",
    "selected_sets = [2, 12, 11, 8, 9, 10, 5]\n",
    "file_template = \"filtered_w10_new_dia_set{}.nc\" #set according your nc files\n",
    "\n",
    "# === Variable Conversion Mapping (name, units, scale factor) ===\n",
    "variables = {\n",
    "    \"diameter_raindrop\": {\"label\": \"Raindrop\", \"unit\": \"µm\", \"scale\": 1e6}, # in micro meter\n",
    "    \"diameter_graupel\": {\"label\": \"Graupel\", \"unit\": \"mm\", \"scale\": 1e3}, # in milimeter\n",
    "    \"diameter_hail\": {\"label\": \"Hail\", \"unit\": \"mm\", \"scale\": 1e3}, # in milimeter\n",
    "}\n",
    "\n",
    "# === Store results grouped by variable\n",
    "stats_by_var = {var: {} for var in variables}\n",
    "\n",
    "# === Loop over datasets\n",
    "for i in selected_sets:\n",
    "    file_path = os.path.join(input_dir, file_template.format(i))\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    for varname, props in variables.items():\n",
    "        if varname not in ds:\n",
    "            print(f\"Variable {varname} not found in set {i}\")\n",
    "            continue\n",
    "\n",
    "        data = ds[varname] * props[\"scale\"]\n",
    "        vals = data.values.flatten()\n",
    "        vals = vals[~np.isnan(vals)]\n",
    "\n",
    "        # Compute statistics\n",
    "        stats = {\n",
    "            \"max\": np.nanmax(vals),\n",
    "            \"p99\": np.nanpercentile(vals, 99),\n",
    "            \"p95\": np.nanpercentile(vals, 95),\n",
    "            \"median\": np.nanmedian(vals),\n",
    "            \"mean\": np.nanmean(vals),\n",
    "        }\n",
    "\n",
    "        stats_by_var[varname][f\"set{i}\"] = stats\n",
    "\n",
    "    ds.close()\n",
    "\n",
    "# === Print grouped by variable\n",
    "for varname, set_stats in stats_by_var.items():\n",
    "    label = variables[varname][\"label\"]\n",
    "    unit = variables[varname][\"unit\"]\n",
    "    print(f\"\\n=== {label} Diameter Stats [{unit}] ===\")\n",
    "    print(f\"{'Dataset':<8}  {'Max':>8}  {'99th':>8}  {'95th':>8}  {'Median':>8}  {'Mean':>8}\")\n",
    "    print(\"-\" * 55)\n",
    "    for dataset, stats in set_stats.items():\n",
    "        print(f\"{dataset:<8}  {stats['max']:8.2f}  {stats['p99']:8.2f}  {stats['p95']:8.2f}  \"\n",
    "              f\"{stats['median']:8.2f}  {stats['mean']:8.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2024b",
   "language": "python",
   "name": "npl-2024b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
